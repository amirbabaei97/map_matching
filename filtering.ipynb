{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing before map-matching\n",
    "### This is a pre-processing stage of the map-matching. We filter the points outside the AOI to reduce the load on the map-matching engine. \n",
    "### A work by Sepehr Rafeie and Amir Babaei\n",
    "\n",
    "###### contact: pr.babayee@icloud.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 216 gzipped files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/25 14:47:09 WARN Utils: Your hostname, relut resolves to a loopback address: 127.0.1.1; using 10.3.0.143 instead (on interface enp3s0)\n",
      "25/03/25 14:47:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Processing CSV files: 100%|██████████| 216/216 [1:30:43<00:00, 25.20s/file]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import gzip\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Load AWS credentials\n",
    "AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\n",
    "AWS_BUCKET = \"inrixprod-trip-reports\"\n",
    "S3_PREFIX = \"user=MB/year=2025/month=01/day=01/date=2025-02-03/reportId=229480/v1/data/waypoints\"\n",
    "\n",
    "# Relut Org S3 for final upload\n",
    "AWS_ACCESS_KEY_RELUT = os.getenv(\"AWS_ACCESS_KEY_RELUT\")\n",
    "AWS_SECRET_KEY_RELUT = os.getenv(\"AWS_SECRET_KEY_RELUT\")\n",
    "AWS_BUCKET_RELUT = os.getenv(\"AWS_BUCKET_RELUT\")\n",
    "\n",
    "# Define temporary storage location (use fast external drive)\n",
    "local_dir = \"/mnt/sda1/waypoints_temp\"\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\"s3\", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)\n",
    "\n",
    "# List all gzipped CSV files\n",
    "response = s3_client.list_objects_v2(Bucket=AWS_BUCKET, Prefix=S3_PREFIX)\n",
    "gz_files = [obj[\"Key\"] for obj in response.get(\"Contents\", []) if obj[\"Key\"].endswith(\".gz\")]\n",
    "print(f\"Found {len(gz_files)} gzipped files.\")\n",
    "\n",
    "# Initialize PySpark with Optimized Settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FCD-Processing\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"96\") \\\n",
    "    .config(\"spark.default.parallelism\", \"48\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"TripId\", StringType(), True),\n",
    "    StructField(\"WaypointSequence\", IntegerType(), True),\n",
    "    StructField(\"CaptureDate\", TimestampType(), True),\n",
    "    StructField(\"Latitude\", DoubleType(), True),\n",
    "    StructField(\"Longitude\", DoubleType(), True),\n",
    "    StructField(\"SegmentId\", StringType(), True),\n",
    "    StructField(\"ZoneName\", StringType(), True),\n",
    "    StructField(\"Frc\", StringType(), True),\n",
    "    StructField(\"DeviceId\", StringType(), True),\n",
    "    StructField(\"RawSpeed\", DoubleType(), True),\n",
    "    StructField(\"RawSpeedMetric\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Bounding Box Limits\n",
    "MIN_LON, MAX_LON, MIN_LAT, MAX_LAT = 8.4, 8.8, 50.0, 50.3\n",
    "\n",
    "# Process files in chunks (batch processing)\n",
    "BATCH_SIZE = 10  # Number of files processed at a time\n",
    "\n",
    "with tqdm(total=len(gz_files), desc=\"Processing CSV files\", unit=\"file\") as pbar:\n",
    "    for i in range(0, len(gz_files), BATCH_SIZE):\n",
    "        batch_files = gz_files[i:i + BATCH_SIZE]\n",
    "        csv_files = []\n",
    "\n",
    "        for s3_file in batch_files:\n",
    "            local_file = os.path.join(local_dir, os.path.basename(s3_file))\n",
    "            extracted_file = local_file.replace(\".gz\", \".csv\")\n",
    "\n",
    "            # Download file\n",
    "            s3_client.download_file(AWS_BUCKET, s3_file, local_file)\n",
    "\n",
    "            # Extract GZ\n",
    "            with gzip.open(local_file, \"rb\") as f_in, open(extracted_file, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "            csv_files.append(extracted_file)\n",
    "            pbar.update(1)  # Update progress bar for each file processed\n",
    "\n",
    "        # Load extracted batch into PySpark\n",
    "        df = spark.read.option(\"maxFilesPerTrigger\", 5).csv(csv_files, schema=schema, header=False)\n",
    "\n",
    "        # Apply BBOX filter\n",
    "        df_filtered = df.filter(\n",
    "            (col(\"Longitude\") >= MIN_LON) & (col(\"Longitude\") <= MAX_LON) &\n",
    "            (col(\"Latitude\") >= MIN_LAT) & (col(\"Latitude\") <= MAX_LAT)\n",
    "        )\n",
    "\n",
    "        # Save batch as Parquet\n",
    "        output_parquet = f\"/mnt/sda1/filtered_fcd/2025/1/filtered_batch_{i//BATCH_SIZE}.parquet\"\n",
    "        df_filtered.write.mode(\"overwrite\").parquet(output_parquet)\n",
    "\n",
    "        \n",
    "        # # Upload processed Parquet to Relut's S3\n",
    "        # s3_client_relut = boto3.client(\"s3\", aws_access_key_id=AWS_ACCESS_KEY_RELUT, aws_secret_access_key=AWS_SECRET_KEY_RELUT)\n",
    "\n",
    "        # for file in tqdm(os.listdir(output_parquet), desc=\"Uploading to S3\", unit=\"file\"):\n",
    "        #     file_path = os.path.join(output_parquet, file)\n",
    "        #     s3_target_key = f\"filtered_data/2024/11/{file}\"\n",
    "\n",
    "        #     s3_client_relut.upload_file(file_path, AWS_BUCKET_RELUT, s3_target_key)\n",
    "\n",
    "        # Cleanup temp files to free space\n",
    "        shutil.rmtree(local_dir)\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "print(\"✅ Processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".map_matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
